# GEOL0069_Week4

<!-- GETTING STARTED -->
## Getting Started

The Week 4 assignment for GEOL0069 Artificial Intelligence for Earth Observation focuses on applying unsupervised learning to a real Earth-observation problem: classifying Sentinel-3 SAR altimetry echoes over polar regions. Each radar measurement contains a returned “echo”  whose shape depends on surface type. In sea-ice zones, echoes can come from sea ice or from leads.  

In this project, we extract a small set of waveform-derived features, then use clustering to separate echoes into two groups. I then summarise the two classes by computing an mean echo shape and a standard deviation envelope. Finally, I evaluate how well the unsupervised clusters match the ESA official classification using a confusion matrix. 

<!-- PREREQUISITES -->
## Prerequisites

Run the notebook in Google Colab.
- Install packages
  ```sh
  !pip install netCDF4
  ```
  ```sh
  !pip install rasterio
  ```
* Mounting Google Drive on Google Colab
  ```sh
  from google.colab import drive
  drive.mount('/content/drive')
  ```

<!-- INTRODUCTION -->
## Introduction to unsupervised learning: K-means Clustering

### What K-means does
K-means is one of the most widely used unsupervised clustering algorithms and provides a clear baseline for thinking about how clustering works. The idea is to partition the dataset into K clusters such that points inside the same cluster are as similar as possible, while points in different clusters are as different as possible. It does this by learning K centroids.

### How the algorithm works
- Each observation is assigned to the closest centroid, and then the centroids are updated to be the mean of the points assigned to them. - This “assign → update” cycle repeats until assignments stop changing or a maximum number of iterations is reached.

### Strengths and limitations for altimetry echoes
- K-means is computationally efficient and easy to implement, but it has limitations that matter for geophysical data.
- It assumes clusters are roughly spherical in feature space and similar in size, and it assigns each point to exactly one cluster.
- The choice of K is also user-defined, so you must decide how many groups exist in advance. In this assignment, while K-means is useful conceptually, a probabilistic approach (GMM) can be more flexible for separating sea-ice and lead populations whose spreads may differ.

## Unsupervised Learning: Gaussian Mixture Models (GMM)

### What a GMM does
- A Gaussian Mixture Model (GMM) treats the dataset as being generated by a mixture of several Gaussian distributions (components). Unlike K-means, which uses only distances to centroids, GMM is probabilistic: each data point receives a probability of belonging to each component.
- This is useful when clusters overlap or have different shapes/spreads. Each Gaussian component has its own mean and covariance, meaning GMM can represent elongated or uneven clusters more naturally than K-means.

### Expectation–Maximization (EM) in practice
- Model parameters are learned using the Expectation–Maximization (EM) algorithm.
- In the E-step, the algorithm estimates membership probabilities for each point. In the M-step, it updates the Gaussian parameters to maximise the likelihood given those responsibilities. This iterative process continues until convergence.

### Why GMM is suitable for sea ice vs lead separation
- In this project, I use a 2-component GMM because the target separation is binary: sea ice vs lead. The input to the model is not the raw waveform but a small set of waveform-derived features.
- After fitting the GMM, each echo is assigned to one of two clusters, which are then interpreted as sea-ice or lead based on the waveform characteristics.

Below is a basic code implementation for a Gaussian Mixture Model.

```sh
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
import numpy as np
import os

# Sample data
X = np.random.rand(100, 2)

# GMM model
gmm = GaussianMixture(n_components=3)
gmm.fit(X)
y_gmm = gmm.predict(X)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=y_gmm, cmap='viridis')
centers = gmm.means_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
plt.title('Gaussian Mixture Model')
```

![Gaussian Mixture Model](GMM.png)
